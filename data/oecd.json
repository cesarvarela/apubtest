{
  "data": {
    "incidents": [
      {
        "incident_id": "OECD-2015-001",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-02-26",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Consumer AI System",
          "version": "3.17.42"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Wikipedia"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "16305 users",
          "economic_losses": "$16.8 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Black people",
          "Workers",
          "Motorists"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2015-002",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-03-10",
        "countries": [
          "South Korea",
          "Netherlands"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine Pro",
          "version": "1.12.79"
        },
        "organizations": {
          "developer": "MIT Media Lab",
          "deployer": "Nest Labs"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "57024 individuals",
          "economic_losses": "$27.1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists",
          "Airplane Passengers"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2015-003",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-03-29",
        "countries": [
          "Italy"
        ],
        "ai_system": {
          "name": "Workplace AI Solution Pro",
          "version": "1.3.58"
        },
        "organizations": {
          "developer": "YouTube",
          "deployer": "Various organizations"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "52771 workers",
          "economic_losses": "$41.1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2015-004",
        "title": "Algorithmic Trading System Triggers Market Manipulation Investigation",
        "description": "High-frequency trading algorithms were found to be creating artificial price movements through coordinated trades. The AI systems learned to exploit market microstructure in ways that resembled illegal manipulation.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-04-01",
        "countries": [
          "United Kingdom",
          "Brazil"
        ],
        "ai_system": {
          "name": "Financial AI System Pro",
          "version": "1.14.45"
        },
        "organizations": {
          "developer": "The DAO",
          "deployer": "New York city Dept. of Education"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Economic/property",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "69747 individuals",
          "economic_losses": "$49.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people"
        ],
        "business_function": [
          "Other (financial trading)"
        ]
      },
      {
        "incident_id": "OECD-2015-005",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-04-07",
        "countries": [
          "Singapore"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "10.12.68"
        },
        "organizations": {
          "developer": "Delphi Technologies",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "17960 users",
          "economic_losses": "$4.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2015-006",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-05-30",
        "countries": [
          "Singapore",
          "France"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "4.15.18"
        },
        "organizations": {
          "developer": "Wikipedia",
          "deployer": "YouTube"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34427 users"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Medical Residents"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2015-007",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-06-04",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance System Pro",
          "version": "7.4.91"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "YouTube"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "85821 workers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people",
          "Bus passengers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2015-008",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-07-11",
        "countries": [
          "United States",
          "China"
        ],
        "ai_system": {
          "name": "Medical AI Platform Pro",
          "version": "4.13.21"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "St George's Hospital Medical School"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "49783 workers",
          "economic_losses": "$44.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients",
          "Minority Groups"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2015-009",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-07-12",
        "countries": [
          "India"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "4.19.87"
        },
        "organizations": {
          "developer": "Frontier Development",
          "deployer": "United States Government"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "90773 workers",
          "economic_losses": "$17.4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people",
          "Pedestrians"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2015-010",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-08-06",
        "countries": [
          "South Korea",
          "United States"
        ],
        "ai_system": {
          "name": "Consumer AI Solution Pro",
          "version": "5.8.43"
        },
        "organizations": {
          "developer": "Northpointe",
          "deployer": "Uber"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "1895 customers",
          "economic_losses": "$10.6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers",
          "Minority Groups",
          "Job applicants"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2015-011",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-08-27",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "2.5.59"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "17547 patients",
          "economic_losses": "$17.3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers",
          "Patients",
          "Children"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2015-012",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-10-02",
        "countries": [
          "Australia",
          "France"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "7.5.72"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "New York city Dept. of Education"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "49767 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women",
          "Workers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2015-013",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-10-20",
        "countries": [
          "Netherlands",
          "Italy"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "8.13.15"
        },
        "organizations": {
          "developer": "Wikipedia",
          "deployer": "Delphi Technologies"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "85473 patients",
          "economic_losses": "$19.6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Bus passengers",
          "Consumers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2015-014",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-10-31",
        "countries": [
          "Sweden",
          "United States"
        ],
        "ai_system": {
          "name": "Consumer AI System Pro",
          "version": "8.3.16"
        },
        "organizations": {
          "developer": "Keolis North America",
          "deployer": "Microsoft"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10402 workers",
          "economic_losses": "$36.1 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2015-015",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-12-23",
        "countries": [
          "Netherlands",
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution",
          "version": "10.11.70"
        },
        "organizations": {
          "developer": "USC Information Sciences Institute",
          "deployer": "LinkedIn"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "52889 individuals",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2015-016",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2015-12-25",
        "countries": [
          "United States",
          "India"
        ],
        "ai_system": {
          "name": "Security/Surveillance Platform Pro",
          "version": "3.17.9"
        },
        "organizations": {
          "developer": "Frontier Development",
          "deployer": "YouTube"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "23101 individuals",
          "economic_losses": "$17.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Consumers",
          "Minority Groups"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2016-017",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-01-24",
        "countries": [
          "Japan",
          "Brazil"
        ],
        "ai_system": {
          "name": "Workplace AI Engine Pro",
          "version": "3.6.96"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "35551 users",
          "economic_losses": "$24.6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2016-018",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-01-25",
        "countries": [
          "France"
        ],
        "ai_system": {
          "name": "Consumer AI Engine Pro",
          "version": "10.17.57"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Amazon"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "58582 individuals",
          "economic_losses": "$25.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Black people",
          "Minority Groups"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2016-019",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-03-06",
        "countries": [
          "India",
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "7.1.55"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Northpointe"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "63418 individuals",
          "economic_losses": "$31.1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2016-020",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-03-23",
        "countries": [
          "Canada",
          "Australia"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "6.0.7"
        },
        "organizations": {
          "developer": "Doctors",
          "deployer": "Apple"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "88463 customers",
          "economic_losses": "$42.6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Black people",
          "Children"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2016-021",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-04-10",
        "countries": [
          "Italy"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "4.11.73"
        },
        "organizations": {
          "developer": "Boeing",
          "deployer": "Navya"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "49732 users",
          "economic_losses": "$38.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Job applicants",
          "Children"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2016-022",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-04-23",
        "countries": [
          "France",
          "Mexico"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "3.15.34"
        },
        "organizations": {
          "developer": "YouTube",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "8741 customers",
          "economic_losses": "$18.3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2016-023",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-05-01",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution Pro",
          "version": "7.9.92"
        },
        "organizations": {
          "developer": "Doctors",
          "deployer": "Frontier Development"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "79684 users",
          "economic_losses": "$36.4 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people",
          "Minority Groups",
          "Pedestrians"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2016-024",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-05-09",
        "countries": [
          "Canada"
        ],
        "ai_system": {
          "name": "Workplace AI Engine Pro",
          "version": "8.17.46"
        },
        "organizations": {
          "developer": "Wikipedia",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "85417 individuals",
          "economic_losses": "$19.8 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Pedestrians",
          "Bus passengers"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2016-025",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-05-23",
        "countries": [
          "Australia",
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI Platform",
          "version": "5.9.2"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Frontier Development"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "48097 patients",
          "economic_losses": "$44.8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients",
          "Pedestrians"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2016-026",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-06-05",
        "countries": [
          "Brazil",
          "Mexico"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "10.6.70"
        },
        "organizations": {
          "developer": "Microsoft Research",
          "deployer": "United States Government"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "98491 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Children",
          "Bus passengers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2016-027",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-06-27",
        "countries": [
          "Japan"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "5.17.29"
        },
        "organizations": {
          "developer": "Microsoft",
          "deployer": "The DAO"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "8243 individuals",
          "economic_losses": "$7.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Students",
          "Airplane Passengers"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2016-028",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-07-12",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution Pro",
          "version": "10.3.20"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Delphi Technologies"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "89313 patients",
          "economic_losses": "$48.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people",
          "Consumers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2016-029",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-08-18",
        "countries": [
          "United States",
          "Italy"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "5.9.60"
        },
        "organizations": {
          "developer": "Boeing",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10853 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Airplane Passengers",
          "Pedestrians"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2016-030",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-08-28",
        "countries": [
          "Norway",
          "Spain"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "10.19.54"
        },
        "organizations": {
          "developer": "Microsoft Research",
          "deployer": "LinkedIn"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "95520 customers",
          "economic_losses": "$32.1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Black people"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2016-031",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-09-08",
        "countries": [
          "South Korea",
          "France"
        ],
        "ai_system": {
          "name": "Workplace AI Engine",
          "version": "9.5.56"
        },
        "organizations": {
          "developer": "Equivant",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10120 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Airplane Passengers",
          "Pedestrians"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2016-032",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-09-30",
        "countries": [
          "Australia"
        ],
        "ai_system": {
          "name": "Security/Surveillance Engine Pro",
          "version": "9.13.82"
        },
        "organizations": {
          "developer": "Boeing",
          "deployer": "Boeing"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34183 patients",
          "economic_losses": "$18.0 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Children",
          "Bus passengers",
          "Black people"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2016-033",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-10-22",
        "countries": [
          "India"
        ],
        "ai_system": {
          "name": "Security/Surveillance System Pro",
          "version": "8.7.3"
        },
        "organizations": {
          "developer": "Equivant",
          "deployer": "St George's Hospital Medical School"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "62307 individuals",
          "economic_losses": "$25.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Children"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2016-034",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2016-10-26",
        "countries": [
          "Sweden"
        ],
        "ai_system": {
          "name": "Consumer AI Engine",
          "version": "10.17.41"
        },
        "organizations": {
          "developer": "The DAO",
          "deployer": "Nest Labs"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "5204 individuals",
          "economic_losses": "$16.8 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Workers",
          "Women",
          "Bus passengers"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2017-035",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-01-13",
        "countries": [
          "France"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine Pro",
          "version": "4.13.73"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Nest Labs"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "76010 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people",
          "Students",
          "Bus passengers"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2017-036",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-01-28",
        "countries": [
          "Mexico",
          "United States"
        ],
        "ai_system": {
          "name": "Consumer AI Solution",
          "version": "9.6.46"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "United States Government"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "58514 workers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Patients"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2017-037",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-03-02",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Consumer AI Platform Pro",
          "version": "9.10.25"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "Google"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "12738 individuals",
          "economic_losses": "$13.1 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women",
          "Bus passengers",
          "Black people"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2017-038",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-03-10",
        "countries": [
          "Germany",
          "Australia"
        ],
        "ai_system": {
          "name": "Workplace AI System",
          "version": "3.7.62"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Starbucks"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34330 workers",
          "economic_losses": "$39.6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Consumers"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2017-039",
        "title": "Algorithmic Trading System Triggers Market Manipulation Investigation",
        "description": "High-frequency trading algorithms were found to be creating artificial price movements through coordinated trades. The AI systems learned to exploit market microstructure in ways that resembled illegal manipulation.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-04-05",
        "countries": [
          "United States",
          "Brazil"
        ],
        "ai_system": {
          "name": "Financial AI Engine",
          "version": "8.4.22"
        },
        "organizations": {
          "developer": "Keolis North America",
          "deployer": "The DAO"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Economic/property",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34265 individuals",
          "economic_losses": "$43.6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Job applicants",
          "Patients"
        ],
        "business_function": [
          "Other (financial trading)"
        ]
      },
      {
        "incident_id": "OECD-2017-040",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-04-08",
        "countries": [
          "Israel",
          "France"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "3.2.94"
        },
        "organizations": {
          "developer": "The DAO",
          "deployer": "Youth Laboratories"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "13099 individuals",
          "economic_losses": "$46.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents",
          "Women",
          "Patients"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2017-041",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-04-09",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine Pro",
          "version": "7.14.76"
        },
        "organizations": {
          "developer": "YouTube",
          "deployer": "Nest Labs"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "2399 workers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women",
          "Black people",
          "Consumers"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2017-042",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-04-21",
        "countries": [
          "Japan",
          "South Korea"
        ],
        "ai_system": {
          "name": "Medical AI Platform",
          "version": "10.12.46"
        },
        "organizations": {
          "developer": "Microsoft",
          "deployer": "Microsoft"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "42200 customers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Children"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2017-043",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-05-13",
        "countries": [
          "Germany"
        ],
        "ai_system": {
          "name": "Workplace AI Engine",
          "version": "6.16.57"
        },
        "organizations": {
          "developer": "Amazon",
          "deployer": "Youth Laboratories"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "92225 customers",
          "economic_losses": "$47.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women",
          "Consumers"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2017-044",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-06-23",
        "countries": [
          "Brazil",
          "France"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "7.19.12"
        },
        "organizations": {
          "developer": "Microsoft Research",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "95313 customers",
          "economic_losses": "$3.1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Women"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2017-045",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-07-29",
        "countries": [
          "India"
        ],
        "ai_system": {
          "name": "Workplace AI System",
          "version": "9.1.48"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Youth Laboratories"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "53245 workers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers",
          "Children",
          "Students"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2017-046",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-09-14",
        "countries": [
          "Norway",
          "Italy"
        ],
        "ai_system": {
          "name": "Medical AI System Pro",
          "version": "8.8.72"
        },
        "organizations": {
          "developer": "Frontier Development",
          "deployer": "Equivant"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "3505 individuals",
          "economic_losses": "$3.5 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2017-047",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-09-23",
        "countries": [
          "Japan",
          "Italy"
        ],
        "ai_system": {
          "name": "Workplace AI Solution Pro",
          "version": "3.7.80"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "38873 customers",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists",
          "Women",
          "Bus passengers"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2017-048",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-10-05",
        "countries": [
          "Germany",
          "Sweden"
        ],
        "ai_system": {
          "name": "Security/Surveillance System Pro",
          "version": "7.0.36"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "Nest Labs"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "68197 individuals",
          "economic_losses": "$24.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people",
          "Bus passengers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2017-049",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-10-09",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Security/Surveillance System",
          "version": "2.13.56"
        },
        "organizations": {
          "developer": "USC Information Sciences Institute",
          "deployer": "Google"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "87535 workers",
          "economic_losses": "$19.2 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women",
          "Minority Groups",
          "Medical Residents"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2017-050",
        "title": "Algorithmic Trading System Triggers Market Manipulation Investigation",
        "description": "High-frequency trading algorithms were found to be creating artificial price movements through coordinated trades. The AI systems learned to exploit market microstructure in ways that resembled illegal manipulation.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2017-11-19",
        "countries": [
          "Italy",
          "Netherlands"
        ],
        "ai_system": {
          "name": "Financial AI System Pro",
          "version": "7.1.17"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Hospitals"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Economic/property",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "97745 customers",
          "economic_losses": "$43.0 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Airplane Passengers",
          "Minority Groups"
        ],
        "business_function": [
          "Other (financial trading)"
        ]
      },
      {
        "incident_id": "OECD-2018-051",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-02-06",
        "countries": [
          "India",
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "5.8.67"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Various organizations"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "11967 users",
          "economic_losses": "$1.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Patients",
          "Motorists"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2018-052",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-03-11",
        "countries": [
          "Sweden"
        ],
        "ai_system": {
          "name": "Autonomous Systems Solution Pro",
          "version": "6.12.39"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Hospitals"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "44667 users",
          "economic_losses": "$36.6 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Workers"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2018-053",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-04-16",
        "countries": [
          "Canada"
        ],
        "ai_system": {
          "name": "Security/Surveillance System",
          "version": "10.7.74"
        },
        "organizations": {
          "developer": "Microsoft Research",
          "deployer": "Google"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "39584 users",
          "economic_losses": "$12.8 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Children",
          "Pedestrians"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2018-054",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-04-23",
        "countries": [
          "France",
          "South Korea"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "8.0.75"
        },
        "organizations": {
          "developer": "Microsoft Research",
          "deployer": "Microsoft"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "55970 individuals",
          "economic_losses": "$26.7 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2018-055",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-04-24",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Workplace AI Solution Pro",
          "version": "7.2.19"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "57560 users",
          "economic_losses": "$9.7 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Students",
          "Children"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2018-056",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-05-15",
        "countries": [
          "Brazil",
          "United States"
        ],
        "ai_system": {
          "name": "Workplace AI Platform Pro",
          "version": "9.4.42"
        },
        "organizations": {
          "developer": "Hospitals",
          "deployer": "Amazon"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "90821 patients",
          "economic_losses": "$43.8 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Black people"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2018-057",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-05-24",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Autonomous Systems Solution Pro",
          "version": "10.12.66"
        },
        "organizations": {
          "developer": "Boeing",
          "deployer": "Amazon"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "20339 patients",
          "economic_losses": "$44.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Airplane Passengers",
          "Medical Residents",
          "Patients"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2018-058",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-05-25",
        "countries": [
          "Netherlands",
          "Brazil"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "7.4.71"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Delphi Technologies"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "67465 patients",
          "economic_losses": "$21.7 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Pedestrians",
          "Students"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2018-059",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-06-08",
        "countries": [
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Autonomous Systems Solution",
          "version": "4.9.91"
        },
        "organizations": {
          "developer": "Doctors",
          "deployer": "Delphi Technologies"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "5735 users",
          "economic_losses": "$34.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Bus passengers",
          "Patients"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2018-060",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-08-03",
        "countries": [
          "Singapore"
        ],
        "ai_system": {
          "name": "Workplace AI System",
          "version": "2.17.56"
        },
        "organizations": {
          "developer": "University of Washington",
          "deployer": "Wikipedia"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "44790 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Airplane Passengers",
          "Medical Residents"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2018-061",
        "title": "Algorithmic Trading System Triggers Market Manipulation Investigation",
        "description": "High-frequency trading algorithms were found to be creating artificial price movements through coordinated trades. The AI systems learned to exploit market microstructure in ways that resembled illegal manipulation.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-08-05",
        "countries": [
          "Canada",
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "10.9.79"
        },
        "organizations": {
          "developer": "Amazon",
          "deployer": "Nest Labs"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Economic/property",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31329 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Minority Groups",
          "Students"
        ],
        "business_function": [
          "Other (financial trading)"
        ]
      },
      {
        "incident_id": "OECD-2018-062",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-08-06",
        "countries": [
          "China"
        ],
        "ai_system": {
          "name": "Security/Surveillance System",
          "version": "1.4.34"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "United States Government"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "73495 workers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2018-063",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-09-07",
        "countries": [
          "Israel",
          "France"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution Pro",
          "version": "5.1.57"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Amazon"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "71086 workers",
          "economic_losses": "$15.0 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Children"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2018-064",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-11-19",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance Engine Pro",
          "version": "4.13.11"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "76282 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Airplane Passengers",
          "Job applicants",
          "Children"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2018-065",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-12-01",
        "countries": [
          "United States",
          "France"
        ],
        "ai_system": {
          "name": "Security/Surveillance System Pro",
          "version": "4.3.57"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Boeing"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "69076 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Bus passengers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2018-066",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-12-14",
        "countries": [
          "Italy",
          "France"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine",
          "version": "2.7.90"
        },
        "organizations": {
          "developer": "Navya",
          "deployer": "New Zealand"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "46911 individuals",
          "economic_losses": "$19.7 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Motorists"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2018-067",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2018-12-30",
        "countries": [
          "Italy"
        ],
        "ai_system": {
          "name": "Workplace AI Engine Pro",
          "version": "4.16.50"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Various organizations"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "80080 individuals",
          "economic_losses": "$39.3 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Children"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2019-068",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-02-18",
        "countries": [
          "Germany",
          "France"
        ],
        "ai_system": {
          "name": "Workplace AI Engine",
          "version": "10.1.26"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "74738 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Women",
          "Students"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2019-069",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-03-15",
        "countries": [
          "Sweden"
        ],
        "ai_system": {
          "name": "Consumer AI System Pro",
          "version": "2.13.3"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Wikipedia"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "5547 customers"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Students"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2019-070",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-03-15",
        "countries": [
          "Germany",
          "Brazil"
        ],
        "ai_system": {
          "name": "Workplace AI System Pro",
          "version": "6.17.60"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "LinkedIn"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "43668 workers",
          "economic_losses": "$25.8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Airplane Passengers"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2019-071",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-04-17",
        "countries": [
          "Spain",
          "France"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution",
          "version": "9.18.14"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "Tesla"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "5224 customers",
          "economic_losses": "$20.3 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients",
          "Airplane Passengers"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2019-072",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-05-03",
        "countries": [
          "Israel"
        ],
        "ai_system": {
          "name": "Consumer AI System Pro",
          "version": "8.5.26"
        },
        "organizations": {
          "developer": "University of Washington",
          "deployer": "Amazon"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "30053 patients",
          "economic_losses": "$43.2 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Motorists"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2019-073",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-05-07",
        "countries": [
          "United Kingdom",
          "France"
        ],
        "ai_system": {
          "name": "Autonomous Systems Solution Pro",
          "version": "4.7.99"
        },
        "organizations": {
          "developer": "Delphi Technologies",
          "deployer": "Boeing"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "50128 patients",
          "economic_losses": "$32.8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Consumers",
          "Minority Groups"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2019-074",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-05-22",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "Workplace AI Platform Pro",
          "version": "5.7.8"
        },
        "organizations": {
          "developer": "Navya",
          "deployer": "Uber"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "1888 patients",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people",
          "Women"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2019-075",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-06-04",
        "countries": [
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Medical AI Platform Pro",
          "version": "3.13.77"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Volkswagen"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "17961 individuals",
          "economic_losses": "$21.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2019-076",
        "title": "Algorithmic Trading System Triggers Market Manipulation Investigation",
        "description": "High-frequency trading algorithms were found to be creating artificial price movements through coordinated trades. The AI systems learned to exploit market microstructure in ways that resembled illegal manipulation.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-06-10",
        "countries": [
          "Norway"
        ],
        "ai_system": {
          "name": "Financial AI Solution",
          "version": "7.10.62"
        },
        "organizations": {
          "developer": "Microsoft",
          "deployer": "Delhi Metro Rail Corporation"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Economic/property",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "4145 workers",
          "economic_losses": "$38.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people"
        ],
        "business_function": [
          "Other (financial trading)"
        ]
      },
      {
        "incident_id": "OECD-2019-077",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-06-23",
        "countries": [
          "France",
          "Brazil"
        ],
        "ai_system": {
          "name": "Consumer AI System Pro",
          "version": "3.1.28"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "The DAO"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "91218 users",
          "economic_losses": "$37.9 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Black people"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2019-078",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-07-03",
        "countries": [
          "Germany",
          "Italy"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine Pro",
          "version": "9.18.85"
        },
        "organizations": {
          "developer": "Microsoft",
          "deployer": "Nest Labs"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "114 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Black people"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2019-079",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-09-07",
        "countries": [
          "United States",
          "Mexico"
        ],
        "ai_system": {
          "name": "Consumer AI Engine Pro",
          "version": "5.16.58"
        },
        "organizations": {
          "developer": "YouTube",
          "deployer": "Various organizations"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "67019 customers",
          "economic_losses": "$30.4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2019-080",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-10-07",
        "countries": [
          "China",
          "Canada"
        ],
        "ai_system": {
          "name": "Security/Surveillance System",
          "version": "2.14.81"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "United States Government"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "19098 individuals",
          "economic_losses": "$49.4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Workers",
          "Students"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2019-081",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-10-19",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Workplace AI System",
          "version": "6.1.31"
        },
        "organizations": {
          "developer": "MIT Media Lab",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "6439 individuals",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Children"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2019-082",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-10-30",
        "countries": [
          "France",
          "Australia"
        ],
        "ai_system": {
          "name": "Workplace AI Platform Pro",
          "version": "7.18.81"
        },
        "organizations": {
          "developer": "Boeing",
          "deployer": "United States Government"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10137 individuals",
          "economic_losses": "$19.9 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2019-083",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-11-06",
        "countries": [
          "Italy"
        ],
        "ai_system": {
          "name": "Consumer AI Solution Pro",
          "version": "1.15.54"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "Various organizations"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "5905 customers",
          "economic_losses": "$19.6 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Students"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2019-084",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-11-09",
        "countries": [
          "Brazil",
          "France"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "1.12.50"
        },
        "organizations": {
          "developer": "St George's Hospital Medical School",
          "deployer": "Northpointe"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "25977 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2019-085",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-11-20",
        "countries": [
          "Japan",
          "Italy"
        ],
        "ai_system": {
          "name": "Workplace AI System Pro",
          "version": "1.2.15"
        },
        "organizations": {
          "developer": "USC Information Sciences Institute",
          "deployer": "Starbucks"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "18857 patients"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Patients"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2019-086",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-12-21",
        "countries": [
          "Spain",
          "Australia"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "6.14.69"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "New Zealand"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "19831 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Airplane Passengers",
          "Pedestrians",
          "Minority Groups"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2019-087",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-12-21",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Consumer AI Solution Pro",
          "version": "10.7.36"
        },
        "organizations": {
          "developer": "YouTube",
          "deployer": "Hospitals"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34247 workers",
          "economic_losses": "$13.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Medical Residents"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2019-088",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2019-12-26",
        "countries": [
          "Netherlands"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine",
          "version": "4.6.28"
        },
        "organizations": {
          "developer": "Northpointe",
          "deployer": "Volkswagen"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "95542 users",
          "economic_losses": "$39.6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists",
          "Medical Residents",
          "Children"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2020-089",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-01-20",
        "countries": [
          "France",
          "China"
        ],
        "ai_system": {
          "name": "Workplace AI Engine Pro",
          "version": "2.11.76"
        },
        "organizations": {
          "developer": "Doctors",
          "deployer": "New York city Dept. of Education"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "88110 customers",
          "economic_losses": "$6.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Medical Residents"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2020-090",
        "title": "AI System Failure Related to AI translation is jeopardizing Afghan asylum claims",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving US Citizenship and Immigration Services. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2020-02-16",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Intelligent Solution",
          "version": "3.0"
        },
        "organizations": {
          "developer": "US Citizenship and Immigration Services",
          "deployer": "US Citizenship and Immigration Services"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "17196 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pashto-speaking asylum seekers",
          "Dari-speaking asylum seekers",
          "anonymous Pashto-speaking refugee"
        ],
        "business_function": [
          "Monitoring and quality control",
          "Compliance and justice"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/532"
      },
      {
        "incident_id": "OECD-2020-091",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-02-18",
        "countries": [
          "India",
          "Sweden"
        ],
        "ai_system": {
          "name": "Security/Surveillance Engine Pro",
          "version": "8.19.6"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Apple"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "79586 individuals",
          "economic_losses": "$28.0 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers",
          "Pedestrians"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2020-092",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-02-20",
        "countries": [
          "United States",
          "Sweden"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine",
          "version": "1.18.2"
        },
        "organizations": {
          "developer": "Amazon",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "72261 workers",
          "economic_losses": "$47.9 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Patients",
          "Pedestrians",
          "Airplane Passengers"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2020-093",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-02-22",
        "countries": [
          "Singapore"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "8.4.33"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "63027 customers",
          "economic_losses": "$2.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2020-094",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-02-28",
        "countries": [
          "Singapore",
          "India"
        ],
        "ai_system": {
          "name": "Workplace AI Platform Pro",
          "version": "1.5.47"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "New York city Dept. of Education"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "87988 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women",
          "Pedestrians"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2020-095",
        "title": "AI System Failure Related to Networking Platform Giggle Employs AI to Determine Users’ Gender, Allegedly Excluding Transgender Women",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Giggle. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-03-27",
        "countries": [
          "Netherlands"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "3.9"
        },
        "organizations": {
          "developer": "Giggle",
          "deployer": "Giggle"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "36706 individuals",
          "economic_losses": "$10 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "women of color",
          "trans women"
        ],
        "business_function": [
          "Marketing and advertisement",
          "ICT management and information security"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/166"
      },
      {
        "incident_id": "OECD-2020-096",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-03-27",
        "countries": [
          "Spain"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "8.17.92"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Boeing"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "87468 workers",
          "economic_losses": "$43.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Women"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2020-097",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-04-02",
        "countries": [
          "Australia"
        ],
        "ai_system": {
          "name": "Security/Surveillance Platform",
          "version": "9.11.66"
        },
        "organizations": {
          "developer": "Frontier Development",
          "deployer": "Delhi Metro Rail Corporation"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "48882 customers",
          "economic_losses": "$43.4 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Bus passengers",
          "Children"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2020-098",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-04-16",
        "countries": [
          "Brazil",
          "France"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "5.13.68"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "LinkedIn"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "45941 users"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents",
          "Motorists",
          "Black people"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2020-099",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-05-10",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Medical AI Solution Pro",
          "version": "7.11.53"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Uber"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "3239 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2020-100",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-05-22",
        "countries": [
          "Israel"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "2.1.36"
        },
        "organizations": {
          "developer": "University of Washington",
          "deployer": "Microsoft"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "40187 individuals",
          "economic_losses": "$41.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Consumers",
          "Children"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2020-101",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-05-25",
        "countries": [
          "India"
        ],
        "ai_system": {
          "name": "Security/Surveillance Platform",
          "version": "9.18.49"
        },
        "organizations": {
          "developer": "Wikipedia",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "35916 patients",
          "economic_losses": "$6.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Children"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2020-102",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-06-15",
        "countries": [
          "France"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "5.1.46"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34480 individuals",
          "economic_losses": "$3.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Airplane Passengers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2020-103",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-07-05",
        "countries": [
          "Netherlands"
        ],
        "ai_system": {
          "name": "Medical AI Platform Pro",
          "version": "7.14.37"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Delhi Metro Rail Corporation"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "48242 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2020-104",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-07-15",
        "countries": [
          "South Korea",
          "United States"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "9.13.44"
        },
        "organizations": {
          "developer": "Frontier Development",
          "deployer": "Various organizations"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "1310 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Medical Residents"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2020-105",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-07-18",
        "countries": [
          "Norway",
          "Sweden"
        ],
        "ai_system": {
          "name": "Workplace AI Engine Pro",
          "version": "9.13.17"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "64515 workers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents",
          "Bus passengers",
          "Consumers"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2020-106",
        "title": "AI System Failure Related to Networking Platform Giggle Employs AI to Determine Users’ Gender, Allegedly Excluding Transgender Women",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Giggle. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2020-09-27",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "1.4"
        },
        "organizations": {
          "developer": "Giggle",
          "deployer": "Giggle"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "23312 individuals",
          "economic_losses": "$2 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "women of color",
          "trans women"
        ],
        "business_function": [
          "Logistics",
          "Maintenance"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/166"
      },
      {
        "incident_id": "OECD-2020-107",
        "title": "AI System Failure Related to Networking Platform Giggle Employs AI to Determine Users’ Gender, Allegedly Excluding Transgender Women",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Giggle. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-10-04",
        "countries": [
          "China"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "2.4"
        },
        "organizations": {
          "developer": "Giggle",
          "deployer": "Giggle"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "30416 individuals",
          "economic_losses": "$4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "women of color",
          "trans women"
        ],
        "business_function": [
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/166"
      },
      {
        "incident_id": "OECD-2020-108",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-10-04",
        "countries": [
          "France",
          "Canada"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "5.18.41"
        },
        "organizations": {
          "developer": "Starbucks",
          "deployer": "Northpointe"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "59118 workers",
          "economic_losses": "$20.1 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Patients"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2020-109",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-10-21",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Workplace AI Solution Pro",
          "version": "5.6.54"
        },
        "organizations": {
          "developer": "Wikipedia",
          "deployer": "Navya"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "87375 patients",
          "economic_losses": "$29.0 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Patients"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2020-110",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-11-04",
        "countries": [
          "Germany"
        ],
        "ai_system": {
          "name": "Medical AI Solution Pro",
          "version": "3.3.10"
        },
        "organizations": {
          "developer": "Microsoft",
          "deployer": "Equivant"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10326 workers",
          "economic_losses": "$49.8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2020-111",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-11-14",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI Solution Pro",
          "version": "8.2.83"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Various organizations"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "40497 customers",
          "economic_losses": "$0.3 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Minority Groups"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2020-112",
        "title": "Algorithmic Trading System Triggers Market Manipulation Investigation",
        "description": "High-frequency trading algorithms were found to be creating artificial price movements through coordinated trades. The AI systems learned to exploit market microstructure in ways that resembled illegal manipulation.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-11-25",
        "countries": [
          "South Korea",
          "Australia"
        ],
        "ai_system": {
          "name": "Financial AI System",
          "version": "10.3.34"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Delphi Technologies"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Economic/property",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "12008 customers",
          "economic_losses": "$31.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups"
        ],
        "business_function": [
          "Other (financial trading)"
        ]
      },
      {
        "incident_id": "OECD-2020-113",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-12-17",
        "countries": [
          "Australia",
          "India"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution Pro",
          "version": "7.8.73"
        },
        "organizations": {
          "developer": "The DAO",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "42879 individuals",
          "economic_losses": "$36.1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients",
          "Children",
          "Black people"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2020-114",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2020-12-28",
        "countries": [
          "Netherlands",
          "Israel"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "3.17.36"
        },
        "organizations": {
          "developer": "Youth Laboratories",
          "deployer": "Microsoft"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "39590 workers",
          "economic_losses": "$7.1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Black people",
          "Women"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2021-115",
        "title": "AI System Failure Related to AI translation is jeopardizing Afghan asylum claims",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving US Citizenship and Immigration Services. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2021-01-17",
        "countries": [
          "Brazil",
          "Canada",
          "Germany"
        ],
        "ai_system": {
          "name": "AI Engine",
          "version": "5.8"
        },
        "organizations": {
          "developer": "US Citizenship and Immigration Services",
          "deployer": "US Citizenship and Immigration Services"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "16621 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Pashto-speaking asylum seekers",
          "Dari-speaking asylum seekers",
          "anonymous Pashto-speaking refugee"
        ],
        "business_function": [
          "Planning and budgeting",
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/532"
      },
      {
        "incident_id": "OECD-2021-116",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-02-09",
        "countries": [
          "Sweden",
          "Norway"
        ],
        "ai_system": {
          "name": "Medical AI Solution",
          "version": "4.0.68"
        },
        "organizations": {
          "developer": "University of Washington",
          "deployer": "LinkedIn"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "50023 users",
          "economic_losses": "$2.5 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Students"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2021-117",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-02-18",
        "countries": [
          "Germany",
          "Italy"
        ],
        "ai_system": {
          "name": "Security/Surveillance System",
          "version": "7.1.19"
        },
        "organizations": {
          "developer": "Northpointe",
          "deployer": "The DAO"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "87990 users",
          "economic_losses": "$33.3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Medical Residents"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2021-118",
        "title": "AI System Failure Related to AI translation is jeopardizing Afghan asylum claims",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving US Citizenship and Immigration Services. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2021-02-26",
        "countries": [
          "France",
          "Israel"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "3.9"
        },
        "organizations": {
          "developer": "US Citizenship and Immigration Services",
          "deployer": "US Citizenship and Immigration Services"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "5496 individuals",
          "economic_losses": "$9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pashto-speaking asylum seekers",
          "Dari-speaking asylum seekers",
          "anonymous Pashto-speaking refugee"
        ],
        "business_function": [
          "Planning and budgeting",
          "Procurement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/532"
      },
      {
        "incident_id": "OECD-2021-119",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-03-08",
        "countries": [
          "Singapore"
        ],
        "ai_system": {
          "name": "Security/Surveillance Platform Pro",
          "version": "1.11.84"
        },
        "organizations": {
          "developer": "Boeing",
          "deployer": "Microsoft"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "9924 customers",
          "economic_losses": "$2.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Airplane Passengers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2021-120",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-03-13",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "Autonomous Systems System Pro",
          "version": "8.5.11"
        },
        "organizations": {
          "developer": "Wikipedia",
          "deployer": "Volkswagen"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "12437 patients",
          "economic_losses": "$14.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Women"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2021-121",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-05-09",
        "countries": [
          "France"
        ],
        "ai_system": {
          "name": "Autonomous Systems Solution",
          "version": "9.12.17"
        },
        "organizations": {
          "developer": "The DAO",
          "deployer": "YouTube"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "29968 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents",
          "Women",
          "Black people"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2021-122",
        "title": "AI System Failure Related to AI translation is jeopardizing Afghan asylum claims",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving US Citizenship and Immigration Services. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2021-05-14",
        "countries": [
          "Sweden",
          "Brazil"
        ],
        "ai_system": {
          "name": "Smart Platform",
          "version": "1.8"
        },
        "organizations": {
          "developer": "US Citizenship and Immigration Services",
          "deployer": "US Citizenship and Immigration Services"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "6006 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pashto-speaking asylum seekers",
          "Dari-speaking asylum seekers",
          "anonymous Pashto-speaking refugee"
        ],
        "business_function": [
          "Marketing and advertisement",
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/532"
      },
      {
        "incident_id": "OECD-2021-123",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-05-31",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "5.6.25"
        },
        "organizations": {
          "developer": "Microsoft",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "74208 workers"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Bus passengers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2021-124",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-06-14",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Consumer AI Platform Pro",
          "version": "9.13.86"
        },
        "organizations": {
          "developer": "MIT Media Lab",
          "deployer": "Boeing"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "16621 individuals",
          "economic_losses": "$27.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Patients"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2021-125",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-06-24",
        "countries": [
          "France",
          "Singapore"
        ],
        "ai_system": {
          "name": "Consumer AI Solution Pro",
          "version": "8.10.40"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Apple"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "11566 patients",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Motorists",
          "Consumers"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2021-126",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-07-08",
        "countries": [
          "China",
          "Germany"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "10.16.31"
        },
        "organizations": {
          "developer": "The DAO",
          "deployer": "Wikipedia"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "79363 users"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Workers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2021-127",
        "title": "Algorithmic Trading System Triggers Market Manipulation Investigation",
        "description": "High-frequency trading algorithms were found to be creating artificial price movements through coordinated trades. The AI systems learned to exploit market microstructure in ways that resembled illegal manipulation.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-08-03",
        "countries": [
          "Singapore"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "5.16.30"
        },
        "organizations": {
          "developer": "Starbucks",
          "deployer": "Apple"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Economic/property",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "57818 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Patients",
          "Minority Groups"
        ],
        "business_function": [
          "Other (financial trading)"
        ]
      },
      {
        "incident_id": "OECD-2021-128",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-08-23",
        "countries": [
          "India",
          "Australia"
        ],
        "ai_system": {
          "name": "Consumer AI Solution",
          "version": "1.5.16"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Volkswagen"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "774 patients",
          "economic_losses": "$10.3 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists",
          "Minority Groups"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2021-129",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-09-13",
        "countries": [
          "France",
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution",
          "version": "10.5.9"
        },
        "organizations": {
          "developer": "MIT Media Lab",
          "deployer": "Navya"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31795 individuals",
          "economic_losses": "$40.4 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Airplane Passengers"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2021-130",
        "title": "AI System Failure Related to Black Uber Eats Driver Allegedly Subjected to Excessive Photo Checks and Dismissed via FRT Results",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Uber Eats. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2021-10-23",
        "countries": [
          "Canada",
          "Italy",
          "United States"
        ],
        "ai_system": {
          "name": "Smart Platform",
          "version": "4.4"
        },
        "organizations": {
          "developer": "Uber Eats",
          "deployer": "Uber Eats"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "43743 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Uber Eats Black delivery drivers",
          "Pa Edrissa Manjang"
        ],
        "business_function": [
          "Human resource management",
          "Marketing and advertisement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/265"
      },
      {
        "incident_id": "OECD-2021-131",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2021-12-08",
        "countries": [
          "Norway"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "10.2.17"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "LinkedIn"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "23712 customers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients",
          "Consumers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2021-132",
        "title": "AI System Failure Related to Black Uber Eats Driver Allegedly Subjected to Excessive Photo Checks and Dismissed via FRT Results",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Uber Eats. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2021-12-24",
        "countries": [
          "Norway"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "2.8"
        },
        "organizations": {
          "developer": "Uber Eats",
          "deployer": "Uber Eats"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "7975 individuals",
          "economic_losses": "$6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Uber Eats Black delivery drivers",
          "Pa Edrissa Manjang"
        ],
        "business_function": [
          "Sales",
          "Procurement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/265"
      },
      {
        "incident_id": "OECD-2022-133",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-02-07",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "9.12.16"
        },
        "organizations": {
          "developer": "University of Washington",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "16711 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents",
          "Job applicants",
          "Motorists"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2022-134",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-02-11",
        "countries": [
          "Netherlands"
        ],
        "ai_system": {
          "name": "Financial AI System",
          "version": "7.18.55"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Boeing"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "17353 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Consumers",
          "Workers"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2022-135",
        "title": "AI System Failure Related to AI translation is jeopardizing Afghan asylum claims",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving US Citizenship and Immigration Services. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2022-02-17",
        "countries": [
          "Netherlands"
        ],
        "ai_system": {
          "name": "Intelligent Engine",
          "version": "4.1"
        },
        "organizations": {
          "developer": "US Citizenship and Immigration Services",
          "deployer": "US Citizenship and Immigration Services"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "15416 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Pashto-speaking asylum seekers",
          "Dari-speaking asylum seekers",
          "anonymous Pashto-speaking refugee"
        ],
        "business_function": [
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/532"
      },
      {
        "incident_id": "OECD-2022-136",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-02-21",
        "countries": [
          "United Kingdom",
          "France"
        ],
        "ai_system": {
          "name": "Security/Surveillance System Pro",
          "version": "2.9.49"
        },
        "organizations": {
          "developer": "USC Information Sciences Institute",
          "deployer": "Google"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "16004 patients",
          "economic_losses": "$11.8 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2022-137",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-03-03",
        "countries": [
          "United States",
          "India"
        ],
        "ai_system": {
          "name": "Consumer AI Engine",
          "version": "6.2.30"
        },
        "organizations": {
          "developer": "Nest Labs",
          "deployer": "Delhi Metro Rail Corporation"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "76725 workers",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2022-138",
        "title": "AI System Failure Related to Starship’s Autonomous Food Delivery Robot Allegedly Stranded at Railroad Crossing in Oregon, Run over by Freight Train",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Oregon State University. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2022-03-05",
        "countries": [
          "Netherlands",
          "France"
        ],
        "ai_system": {
          "name": "Automated Solution",
          "version": "2.5"
        },
        "organizations": {
          "developer": "Oregon State University",
          "deployer": "Oregon State University"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "48388 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Oregon State University",
          "freight train crew"
        ],
        "business_function": [
          "Marketing and advertisement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/176"
      },
      {
        "incident_id": "OECD-2022-139",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-03-14",
        "countries": [
          "Canada",
          "Singapore"
        ],
        "ai_system": {
          "name": "Medical AI System Pro",
          "version": "10.10.36"
        },
        "organizations": {
          "developer": "Keolis North America",
          "deployer": "Google"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "63327 individuals",
          "economic_losses": "$3.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists",
          "Consumers",
          "Bus passengers"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2022-140",
        "title": "AI System Failure Related to Fake LinkedIn Profiles Created Using GAN Photos",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2022-04-02",
        "countries": [
          "Sweden",
          "United States",
          "United Kingdom"
        ],
        "ai_system": {
          "name": "AI Solution",
          "version": "1.1"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "7005 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "LinkedIn users"
        ],
        "business_function": [
          "Research and development",
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/174"
      },
      {
        "incident_id": "OECD-2022-141",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-04-02",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Medical AI System Pro",
          "version": "6.7.17"
        },
        "organizations": {
          "developer": "Doctors",
          "deployer": "LinkedIn"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "22063 customers",
          "economic_losses": "$12.7 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Bus passengers",
          "Children"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2022-142",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-04-30",
        "countries": [
          "Italy"
        ],
        "ai_system": {
          "name": "Autonomous Systems Platform",
          "version": "7.16.70"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Volkswagen"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "57456 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Students",
          "Minority Groups"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2022-143",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-06-04",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "10.12.9"
        },
        "organizations": {
          "developer": "Delphi Technologies",
          "deployer": "Delhi Metro Rail Corporation"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "1533 workers",
          "economic_losses": "$3.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2022-144",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-06-08",
        "countries": [
          "Germany"
        ],
        "ai_system": {
          "name": "Autonomous Systems Solution",
          "version": "5.15.28"
        },
        "organizations": {
          "developer": "USC Information Sciences Institute",
          "deployer": "Tesla"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "76166 patients",
          "economic_losses": "$24.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2022-145",
        "title": "AI System Failure Related to Stable Diffusion Abused by 4chan Users to Deepfake Celebrity Porn",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Stability AI. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-06-20",
        "countries": [
          "Mexico",
          "France"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "2.3"
        },
        "organizations": {
          "developer": "Stability AI",
          "deployer": "Stability AI"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "40619 individuals",
          "economic_losses": "$3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Stability AI",
          "deepfaked celebrities"
        ],
        "business_function": [
          "Compliance and justice",
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/314"
      },
      {
        "incident_id": "OECD-2022-146",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-06-29",
        "countries": [
          "Spain",
          "Sweden"
        ],
        "ai_system": {
          "name": "Security/Surveillance Engine",
          "version": "6.9.11"
        },
        "organizations": {
          "developer": "Nest Labs",
          "deployer": "Boeing"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "73206 users",
          "economic_losses": "$1.1 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2022-147",
        "title": "AI System Failure Related to Networking Platform Giggle Employs AI to Determine Users’ Gender, Allegedly Excluding Transgender Women",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Giggle. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-07-01",
        "countries": [
          "Israel",
          "Netherlands"
        ],
        "ai_system": {
          "name": "Automated System",
          "version": "3.0"
        },
        "organizations": {
          "developer": "Giggle",
          "deployer": "Giggle"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "13476 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "women of color",
          "trans women"
        ],
        "business_function": [
          "Logistics",
          "Accounting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/166"
      },
      {
        "incident_id": "OECD-2022-148",
        "title": "AI System Failure Related to Fake LinkedIn Profiles Created Using GAN Photos",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-07-03",
        "countries": [
          "Brazil",
          "Spain"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "5.7"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "26156 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "LinkedIn users"
        ],
        "business_function": [
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/174"
      },
      {
        "incident_id": "OECD-2022-149",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-07-19",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "8.13.87"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Delhi Metro Rail Corporation"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "8260 users",
          "economic_losses": "$1.8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2022-150",
        "title": "AI System Failure Related to AI translation is jeopardizing Afghan asylum claims",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving US Citizenship and Immigration Services. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2022-08-05",
        "countries": [
          "Israel",
          "Norway",
          "Brazil"
        ],
        "ai_system": {
          "name": "AI Engine",
          "version": "2.9"
        },
        "organizations": {
          "developer": "US Citizenship and Immigration Services",
          "deployer": "US Citizenship and Immigration Services"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "14971 individuals",
          "economic_losses": "$7 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pashto-speaking asylum seekers",
          "Dari-speaking asylum seekers",
          "anonymous Pashto-speaking refugee"
        ],
        "business_function": [
          "Maintenance"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/532"
      },
      {
        "incident_id": "OECD-2022-151",
        "title": "AI System Failure Related to Networking Platform Giggle Employs AI to Determine Users’ Gender, Allegedly Excluding Transgender Women",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Giggle. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Journalist",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2022-08-21",
        "countries": [
          "Japan"
        ],
        "ai_system": {
          "name": "Smart Engine",
          "version": "3.8"
        },
        "organizations": {
          "developer": "Giggle",
          "deployer": "Giggle"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "27496 individuals",
          "economic_losses": "$5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "women of color",
          "trans women"
        ],
        "business_function": [
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/166"
      },
      {
        "incident_id": "OECD-2022-152",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-10-19",
        "countries": [
          "Israel"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "6.12.40"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Frontier Development"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "60484 individuals",
          "economic_losses": "$16.7 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Consumers",
          "Motorists"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2022-153",
        "title": "AI System Failure Related to Black Uber Eats Driver Allegedly Subjected to Excessive Photo Checks and Dismissed via FRT Results",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Uber Eats. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2022-11-13",
        "countries": [
          "Singapore",
          "Norway"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "3.1"
        },
        "organizations": {
          "developer": "Uber Eats",
          "deployer": "Uber Eats"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "2860 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Uber Eats Black delivery drivers",
          "Pa Edrissa Manjang"
        ],
        "business_function": [
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/265"
      },
      {
        "incident_id": "OECD-2022-154",
        "title": "AI System Failure Related to Stable Diffusion Abused by 4chan Users to Deepfake Celebrity Porn",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Stability AI. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2022-11-13",
        "countries": [
          "Norway",
          "Israel",
          "Spain"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "2.8"
        },
        "organizations": {
          "developer": "Stability AI",
          "deployer": "Stability AI"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "14257 individuals",
          "economic_losses": "$1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Stability AI",
          "deepfaked celebrities"
        ],
        "business_function": [
          "Logistics",
          "ICT management and information security"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/314"
      },
      {
        "incident_id": "OECD-2022-155",
        "title": "AI System Failure Related to Networking Platform Giggle Employs AI to Determine Users’ Gender, Allegedly Excluding Transgender Women",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Giggle. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-11-25",
        "countries": [
          "Brazil",
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Smart Platform",
          "version": "3.0"
        },
        "organizations": {
          "developer": "Giggle",
          "deployer": "Giggle"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "39976 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "women of color",
          "trans women"
        ],
        "business_function": [
          "Planning and budgeting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/166"
      },
      {
        "incident_id": "OECD-2022-156",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-11-25",
        "countries": [
          "France"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution",
          "version": "10.18.62"
        },
        "organizations": {
          "developer": "Keolis North America",
          "deployer": "Boeing"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "8861 patients",
          "economic_losses": "$23.3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Women",
          "Motorists"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2022-157",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2022-12-09",
        "countries": [
          "Spain",
          "Israel"
        ],
        "ai_system": {
          "name": "Workplace AI System Pro",
          "version": "2.6.58"
        },
        "organizations": {
          "developer": "Keolis North America",
          "deployer": "YouTube"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "12079 customers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Workers"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2023-158",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-01-06",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI Solution Pro",
          "version": "5.12.78"
        },
        "organizations": {
          "developer": "St George's Hospital Medical School",
          "deployer": "Northpointe"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "84740 customers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2023-159",
        "title": "AI System Failure Related to Tesla Autopilot Misidentified On-Road Horse-Drawn Carriage",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Tesla. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Journalist",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-01-15",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "Intelligent Platform",
          "version": "4.6"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "40711 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Tesla drivers",
          "horse-drawn carriages"
        ],
        "business_function": [
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/398"
      },
      {
        "incident_id": "OECD-2023-160",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-01-15",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Consumer AI Platform Pro",
          "version": "5.17.11"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "United States Government"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "4353 individuals",
          "economic_losses": "$14.5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Black people"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2023-161",
        "title": "Agricultural AI Misidentifies Crops Leading to Herbicide Damage",
        "description": "An AI-powered crop monitoring and treatment system misidentified valuable crops as weeds, resulting in herbicide application that destroyed thousands of acres of produce. The computer vision model was not properly validated for the specific crop varieties.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-02-17",
        "countries": [
          "Mexico",
          "China"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "1.13.12"
        },
        "organizations": {
          "developer": "Boston University",
          "deployer": "Volkswagen"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Environmental"
        ],
        "harm_quantification": {
          "affected_stakeholders": "96201 users",
          "economic_losses": "$11.8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Workers"
        ],
        "business_function": [
          "Production",
          "Other (agriculture)"
        ]
      },
      {
        "incident_id": "OECD-2023-162",
        "title": "AI System Failure Related to Black Uber Eats Driver Allegedly Subjected to Excessive Photo Checks and Dismissed via FRT Results",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Uber Eats. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2023-03-13",
        "countries": [
          "Spain",
          "Italy"
        ],
        "ai_system": {
          "name": "Smart Engine",
          "version": "1.3"
        },
        "organizations": {
          "developer": "Uber Eats",
          "deployer": "Uber Eats"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31334 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Uber Eats Black delivery drivers",
          "Pa Edrissa Manjang"
        ],
        "business_function": [
          "Logistics"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/265"
      },
      {
        "incident_id": "OECD-2023-163",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-03-16",
        "countries": [
          "Italy"
        ],
        "ai_system": {
          "name": "Financial AI Solution Pro",
          "version": "5.14.84"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "Northpointe"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "84941 individuals",
          "economic_losses": "$47.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2023-164",
        "title": "AI System Failure Related to Starship’s Autonomous Food Delivery Robot Allegedly Stranded at Railroad Crossing in Oregon, Run over by Freight Train",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Oregon State University. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-05-04",
        "countries": [
          "India",
          "Sweden"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "2.8"
        },
        "organizations": {
          "developer": "Oregon State University",
          "deployer": "Oregon State University"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "29518 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Oregon State University",
          "freight train crew"
        ],
        "business_function": [
          "Compliance and justice",
          "Procurement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/176"
      },
      {
        "incident_id": "OECD-2023-165",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-05-08",
        "countries": [
          "United States",
          "Brazil"
        ],
        "ai_system": {
          "name": "Autonomous Systems Platform Pro",
          "version": "6.3.27"
        },
        "organizations": {
          "developer": "St George's Hospital Medical School",
          "deployer": "Starbucks"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "27018 workers",
          "economic_losses": "$36.4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Motorists"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2023-166",
        "title": "AI System Failure Related to Black Uber Eats Driver Allegedly Subjected to Excessive Photo Checks and Dismissed via FRT Results",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Uber Eats. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-05-10",
        "countries": [
          "South Korea",
          "Norway"
        ],
        "ai_system": {
          "name": "AI Engine",
          "version": "3.1"
        },
        "organizations": {
          "developer": "Uber Eats",
          "deployer": "Uber Eats"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "22668 individuals",
          "economic_losses": "$4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Uber Eats Black delivery drivers",
          "Pa Edrissa Manjang"
        ],
        "business_function": [
          "Sales"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/265"
      },
      {
        "incident_id": "OECD-2023-167",
        "title": "AI System Failure Related to Tesla Autopilot Misidentified On-Road Horse-Drawn Carriage",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Tesla. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2023-05-28",
        "countries": [
          "India"
        ],
        "ai_system": {
          "name": "Intelligent Engine",
          "version": "2.1"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "38734 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Tesla drivers",
          "horse-drawn carriages"
        ],
        "business_function": [
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/398"
      },
      {
        "incident_id": "OECD-2023-168",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-05-29",
        "countries": [
          "China"
        ],
        "ai_system": {
          "name": "Medical AI System Pro",
          "version": "3.1.60"
        },
        "organizations": {
          "developer": "Keolis North America",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "90692 workers",
          "economic_losses": "$39.8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2023-169",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-06-04",
        "countries": [
          "Israel",
          "Norway"
        ],
        "ai_system": {
          "name": "Medical AI Platform",
          "version": "5.13.17"
        },
        "organizations": {
          "developer": "Keolis North America",
          "deployer": "Youth Laboratories"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "4560 workers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2023-170",
        "title": "AI System Failure Related to Tesla Autopilot Misidentified On-Road Horse-Drawn Carriage",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Tesla. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-06-20",
        "countries": [
          "Sweden",
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "2.2"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31734 individuals",
          "economic_losses": "$3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Tesla drivers",
          "horse-drawn carriages"
        ],
        "business_function": [
          "Citizen/customer service",
          "Planning and budgeting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/398"
      },
      {
        "incident_id": "OECD-2023-171",
        "title": "AI System Failure Related to Black Uber Eats Driver Allegedly Subjected to Excessive Photo Checks and Dismissed via FRT Results",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Uber Eats. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2023-07-19",
        "countries": [
          "Canada",
          "Italy"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "3.6"
        },
        "organizations": {
          "developer": "Uber Eats",
          "deployer": "Uber Eats"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "32078 individuals",
          "economic_losses": "$5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Uber Eats Black delivery drivers",
          "Pa Edrissa Manjang"
        ],
        "business_function": [
          "Citizen/customer service",
          "Procurement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/265"
      },
      {
        "incident_id": "OECD-2023-172",
        "title": "AI System Failure Related to AI-Generated Fake Audio of Verbal Abuse Incident Circulates of British Labour Leader Keir Starmer",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2023-08-03",
        "countries": [
          "Netherlands",
          "Spain",
          "United States"
        ],
        "ai_system": {
          "name": "AI System",
          "version": "5.0"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "45023 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "UK Labour Party",
          "Keir Starmer"
        ],
        "business_function": [
          "Citizen/customer service"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/601"
      },
      {
        "incident_id": "OECD-2023-173",
        "title": "AI System Failure Related to Tesla Autopilot Misidentified On-Road Horse-Drawn Carriage",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Tesla. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-08-07",
        "countries": [
          "France",
          "Germany"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "5.2"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "40211 individuals",
          "economic_losses": "$2 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Tesla drivers",
          "horse-drawn carriages"
        ],
        "business_function": [
          "Accounting",
          "Research and development"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/398"
      },
      {
        "incident_id": "OECD-2023-174",
        "title": "AI System Failure Related to Female Celebrities' Faces Shown in Sexually Suggestive Ads for Deepfake App",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Facemega. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-08-25",
        "countries": [
          "South Korea",
          "France"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "1.3"
        },
        "organizations": {
          "developer": "Facemega",
          "deployer": "Facemega"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "48236 individuals",
          "economic_losses": "$3 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Scarlett Johansson",
          "female celebrities",
          "Emma Watson"
        ],
        "business_function": [
          "Marketing and advertisement",
          "Research and development"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/494"
      },
      {
        "incident_id": "OECD-2023-175",
        "title": "AI System Failure Related to Fake LinkedIn Profiles Created Using GAN Photos",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2023-08-30",
        "countries": [
          "Canada",
          "Sweden"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "2.1"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "18392 individuals",
          "economic_losses": "$9 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "LinkedIn users"
        ],
        "business_function": [
          "Production",
          "Research and development"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/174"
      },
      {
        "incident_id": "OECD-2023-176",
        "title": "AI System Failure Related to Glovo Driver in Italy Fired via Automated Email after Being Killed in Accident",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Glovo. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2023-09-23",
        "countries": [
          "Israel"
        ],
        "ai_system": {
          "name": "Automated System",
          "version": "2.1"
        },
        "organizations": {
          "developer": "Glovo",
          "deployer": "Glovo"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "8511 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Sebastian Galassi's family",
          "Sebastian Galassi"
        ],
        "business_function": [
          "Marketing and advertisement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/384"
      },
      {
        "incident_id": "OECD-2023-177",
        "title": "AI System Failure Related to Stable Diffusion Abused by 4chan Users to Deepfake Celebrity Porn",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Stability AI. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Journalist",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2023-09-24",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "5.9"
        },
        "organizations": {
          "developer": "Stability AI",
          "deployer": "Stability AI"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "16459 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Stability AI",
          "deepfaked celebrities"
        ],
        "business_function": [
          "Accounting",
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/314"
      },
      {
        "incident_id": "OECD-2023-178",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-09-24",
        "countries": [
          "Spain",
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "9.8.73"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "Google"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "81772 users",
          "economic_losses": "$21.2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents",
          "Job applicants",
          "Patients"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2023-179",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-10-06",
        "countries": [
          "China"
        ],
        "ai_system": {
          "name": "Autonomous Systems Engine Pro",
          "version": "7.4.37"
        },
        "organizations": {
          "developer": "Apple",
          "deployer": "United States Government"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "5934 workers",
          "economic_losses": "$9.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Women"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2023-180",
        "title": "AI System Failure Related to Bankrate's Resumption of AI-Generated Content Allegedly Continuing to Produce Inaccurate and Misleading Information",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Red Ventures. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2023-10-29",
        "countries": [
          "France",
          "Australia",
          "Germany"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "5.1"
        },
        "organizations": {
          "developer": "Red Ventures",
          "deployer": "Red Ventures"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "8615 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Journalistic integrity",
          "General public"
        ],
        "business_function": [
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/577"
      },
      {
        "incident_id": "OECD-2023-181",
        "title": "AI System Failure Related to Fake LinkedIn Profiles Created Using GAN Photos",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2023-12-02",
        "countries": [
          "France",
          "Norway",
          "Spain"
        ],
        "ai_system": {
          "name": "Intelligent Solution",
          "version": "5.2"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "3102 individuals",
          "economic_losses": "$8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "LinkedIn users"
        ],
        "business_function": [
          "Planning and budgeting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/174"
      },
      {
        "incident_id": "OECD-2023-182",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-12-07",
        "countries": [
          "Israel"
        ],
        "ai_system": {
          "name": "Medical AI Engine",
          "version": "4.7.59"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "Navya"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "18347 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Black people",
          "Motorists"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2023-183",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-12-26",
        "countries": [
          "Sweden"
        ],
        "ai_system": {
          "name": "Financial AI Engine Pro",
          "version": "10.3.46"
        },
        "organizations": {
          "developer": "Navya",
          "deployer": "Delphi Technologies"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "67251 users",
          "economic_losses": "$26.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Bus passengers",
          "Minority Groups"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2023-184",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2023-12-27",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution Pro",
          "version": "6.10.73"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "Northpointe"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "30474 customers",
          "economic_losses": "$12.9 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Motorists"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2023-185",
        "title": "AI System Failure Related to Stable Diffusion Abused by 4chan Users to Deepfake Celebrity Porn",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Stability AI. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2023-12-30",
        "countries": [
          "France",
          "South Korea"
        ],
        "ai_system": {
          "name": "Smart Platform",
          "version": "3.3"
        },
        "organizations": {
          "developer": "Stability AI",
          "deployer": "Stability AI"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "21725 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Stability AI",
          "deepfaked celebrities"
        ],
        "business_function": [
          "Marketing and advertisement",
          "Planning and budgeting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/314"
      },
      {
        "incident_id": "OECD-2024-186",
        "title": "AI System Failure Related to Female Celebrities' Faces Shown in Sexually Suggestive Ads for Deepfake App",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Facemega. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2024-01-02",
        "countries": [
          "United Kingdom",
          "Germany"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "3.7"
        },
        "organizations": {
          "developer": "Facemega",
          "deployer": "Facemega"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "3292 individuals",
          "economic_losses": "$1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Scarlett Johansson",
          "female celebrities",
          "Emma Watson"
        ],
        "business_function": [
          "Procurement",
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/494"
      },
      {
        "incident_id": "OECD-2024-187",
        "title": "Employee Monitoring AI Incorrectly Flags Remote Workers as Unproductive",
        "description": "An AI-powered employee productivity monitoring system incorrectly categorized focused work activities as idle time, leading to unfair performance reviews and terminations. The system failed to understand different working styles and break patterns.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-01-17",
        "countries": [
          "Norway",
          "Japan"
        ],
        "ai_system": {
          "name": "Workplace AI System",
          "version": "5.2.54"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "Uber"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Psychological",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "2442 workers",
          "economic_losses": "$0.3 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers",
          "Medical Residents"
        ],
        "business_function": [
          "Human resource management",
          "Monitoring and quality control"
        ]
      },
      {
        "incident_id": "OECD-2024-188",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-01-19",
        "countries": [
          "Netherlands",
          "India"
        ],
        "ai_system": {
          "name": "Consumer AI Engine",
          "version": "6.12.8"
        },
        "organizations": {
          "developer": "Nest Labs",
          "deployer": "LinkedIn"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "92624 customers",
          "economic_losses": "$25.8 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children",
          "Consumers"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2024-189",
        "title": "AI System Failure Related to Tesla Autopilot Misidentified On-Road Horse-Drawn Carriage",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Tesla. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-01-31",
        "countries": [
          "Spain"
        ],
        "ai_system": {
          "name": "Intelligent Solution",
          "version": "5.4"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "45412 individuals",
          "economic_losses": "$10 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Tesla drivers",
          "horse-drawn carriages"
        ],
        "business_function": [
          "Sales",
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/398"
      },
      {
        "incident_id": "OECD-2024-190",
        "title": "AI System Failure Related to Starship’s Autonomous Food Delivery Robot Allegedly Stranded at Railroad Crossing in Oregon, Run over by Freight Train",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Oregon State University. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2024-01-31",
        "countries": [
          "Netherlands"
        ],
        "ai_system": {
          "name": "Intelligent Platform",
          "version": "5.2"
        },
        "organizations": {
          "developer": "Oregon State University",
          "deployer": "Oregon State University"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31361 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Oregon State University",
          "freight train crew"
        ],
        "business_function": [
          "Accounting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/176"
      },
      {
        "incident_id": "OECD-2024-191",
        "title": "AI System Failure Related to Class Action Lawsuit Over Alleged Defects in Volkswagen's AI-Driven AEB Systems",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Volkswagen Group of America. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2024-02-02",
        "countries": [
          "Spain",
          "United States"
        ],
        "ai_system": {
          "name": "Intelligent Solution",
          "version": "3.2"
        },
        "organizations": {
          "developer": "Volkswagen Group of America",
          "deployer": "Volkswagen Group of America"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34896 individuals",
          "economic_losses": "$1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Volkswagen drivers",
          "Potential passengers and road users at risk due to malfunctioning AEB systems"
        ],
        "business_function": [
          "Sales",
          "Planning and budgeting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/746"
      },
      {
        "incident_id": "OECD-2024-192",
        "title": "AI System Failure Related to Fake LinkedIn Profiles Created Using GAN Photos",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-02-07",
        "countries": [
          "United States",
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "2.2"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "24793 individuals",
          "economic_losses": "$4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "LinkedIn users"
        ],
        "business_function": [
          "ICT management and information security",
          "Compliance and justice"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/174"
      },
      {
        "incident_id": "OECD-2024-193",
        "title": "AI System Failure Related to Glovo Driver in Italy Fired via Automated Email after Being Killed in Accident",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Glovo. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2024-02-10",
        "countries": [
          "France",
          "Australia",
          "Japan"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "3.0"
        },
        "organizations": {
          "developer": "Glovo",
          "deployer": "Glovo"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "29488 individuals",
          "economic_losses": "$2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Sebastian Galassi's family",
          "Sebastian Galassi"
        ],
        "business_function": [
          "Production",
          "Sales"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/384"
      },
      {
        "incident_id": "OECD-2024-194",
        "title": "AI System Failure Related to British Female Politicians Victimized by Deepfake Pornography",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Unknown deepfake creators. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2024-02-20",
        "countries": [
          "United Kingdom",
          "France",
          "Netherlands"
        ],
        "ai_system": {
          "name": "Intelligent Engine",
          "version": "1.9"
        },
        "organizations": {
          "developer": "Unknown deepfake creators",
          "deployer": "Unknown deepfake creators"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "42160 individuals",
          "economic_losses": "$2 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Stella Creasy",
          "Priti Patel",
          "Penny Mordaunt",
          "Gillian Keegan",
          "Dehenna Davison",
          "Angela Rayner"
        ],
        "business_function": [
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/754"
      },
      {
        "incident_id": "OECD-2024-195",
        "title": "AI System Failure Related to British Female Politicians Victimized by Deepfake Pornography",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Unknown deepfake creators. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-02-21",
        "countries": [
          "United States",
          "Canada",
          "South Korea"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "2.7"
        },
        "organizations": {
          "developer": "Unknown deepfake creators",
          "deployer": "Unknown deepfake creators"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "6480 individuals",
          "economic_losses": "$3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Stella Creasy",
          "Priti Patel",
          "Penny Mordaunt",
          "Gillian Keegan",
          "Dehenna Davison",
          "Angela Rayner"
        ],
        "business_function": [
          "Procurement",
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/754"
      },
      {
        "incident_id": "OECD-2024-196",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-05-25",
        "countries": [
          "Sweden",
          "Germany"
        ],
        "ai_system": {
          "name": "Security/Surveillance Engine Pro",
          "version": "1.5.24"
        },
        "organizations": {
          "developer": "Doctors",
          "deployer": "United States Government"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "73973 individuals",
          "economic_losses": "$6.3 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Pedestrians",
          "Patients",
          "Motorists"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2024-197",
        "title": "AI Credit Scoring System Discriminates Against Recent Immigrants",
        "description": "An automated credit scoring system systematically assigned lower credit scores to recent immigrants despite strong financial indicators. The AI model heavily weighted credit history length, creating unfair barriers to financial services.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Digital Rights Organization",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-05-31",
        "countries": [
          "Singapore"
        ],
        "ai_system": {
          "name": "Financial AI Engine",
          "version": "4.4.4"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Amazon"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "614 customers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Motorists"
        ],
        "business_function": [
          "Other (financial services)"
        ]
      },
      {
        "incident_id": "OECD-2024-198",
        "title": "AI Interview System Shows Bias Against Non-Native Speakers",
        "description": "An automated video interview analysis system consistently gave lower scores to qualified candidates with accents or non-native speech patterns. The AI's speech recognition and sentiment analysis components showed significant bias.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-06-17",
        "countries": [
          "Canada",
          "France"
        ],
        "ai_system": {
          "name": "Workplace AI System",
          "version": "4.16.87"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "New York city Dept. of Education"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Economic/property",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "39301 workers",
          "economic_losses": "$5.5 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Job applicants"
        ],
        "business_function": [
          "Human resource management"
        ]
      },
      {
        "incident_id": "OECD-2024-199",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-06-26",
        "countries": [
          "France",
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance System",
          "version": "4.6.36"
        },
        "organizations": {
          "developer": "YouTube",
          "deployer": "Google"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "79021 individuals",
          "economic_losses": "$11.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2024-200",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-06-26",
        "countries": [
          "Norway",
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "6.14.51"
        },
        "organizations": {
          "developer": "Frontier Development",
          "deployer": "Google"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "64254 customers"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Minority Groups",
          "Black people"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2024-201",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "University Ethics Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-06-27",
        "countries": [
          "Italy",
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "2.1.77"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Hospitals"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "63580 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2024-202",
        "title": "Delivery Robot Swarm Blocks Emergency Vehicle Access",
        "description": "A fleet of autonomous delivery robots failed to coordinate their movements during an emergency, blocking paramedic access to a medical emergency. The swarm intelligence system lacked proper emergency vehicle detection and response protocols.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-07-05",
        "countries": [
          "Italy",
          "United States"
        ],
        "ai_system": {
          "name": "Autonomous Systems System",
          "version": "9.18.1"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "Tesla"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Public interest/critical infrastructure"
        ],
        "harm_quantification": {
          "affected_stakeholders": "50380 users",
          "economic_losses": "$33.7 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Logistics",
          "Other (delivery services)"
        ]
      },
      {
        "incident_id": "OECD-2024-203",
        "title": "AI System Failure Related to AI Chatbots Reportedly Inaccurately Conveyed Real-Time Political News",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Perplexity. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-07-31",
        "countries": [
          "Spain",
          "Australia"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "2.5"
        },
        "organizations": {
          "developer": "Perplexity",
          "deployer": "Perplexity"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "18033 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Journalism",
          "General public",
          "Chatbot users"
        ],
        "business_function": [
          "Production",
          "Maintenance"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/750"
      },
      {
        "incident_id": "OECD-2024-204",
        "title": "AI Personal Assistant Shares Private Information in Group Settings",
        "description": "A voice-activated AI assistant failed to recognize multi-user environments and shared personal calendar entries, medical reminders, and financial information when responding to queries in group settings.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-08-01",
        "countries": [
          "Spain"
        ],
        "ai_system": {
          "name": "Consumer AI Platform Pro",
          "version": "6.17.31"
        },
        "organizations": {
          "developer": "Volkswagen",
          "deployer": "The DAO"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Reputational",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "41589 individuals",
          "economic_losses": "$11.3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Pedestrians",
          "Workers"
        ],
        "business_function": [
          "Citizen/customer service",
          "ICT management and information security"
        ]
      },
      {
        "incident_id": "OECD-2024-205",
        "title": "Smart Home AI Creates Dangerous Living Conditions for Elderly",
        "description": "An AI-powered smart home system learned incorrect patterns from an elderly resident with dementia, subsequently creating dangerous conditions by adjusting heating, lighting, and security settings inappropriately.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-08-10",
        "countries": [
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Consumer AI Engine",
          "version": "6.0.62"
        },
        "organizations": {
          "developer": "YouTube",
          "deployer": "The DAO"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "94152 workers",
          "economic_losses": "$25.2 million",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Workers",
          "Motorists"
        ],
        "business_function": [
          "Citizen/customer service",
          "Other (home automation)"
        ]
      },
      {
        "incident_id": "OECD-2024-206",
        "title": "AI System Failure Related to Class Action Lawsuit Over Alleged Defects in Volkswagen's AI-Driven AEB Systems",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Volkswagen Group of America. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-08-11",
        "countries": [
          "Norway"
        ],
        "ai_system": {
          "name": "Intelligent Engine",
          "version": "5.1"
        },
        "organizations": {
          "developer": "Volkswagen Group of America",
          "deployer": "Volkswagen Group of America"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "49847 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Volkswagen drivers",
          "Potential passengers and road users at risk due to malfunctioning AEB systems"
        ],
        "business_function": [
          "Citizen/customer service",
          "Marketing and advertisement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/746"
      },
      {
        "incident_id": "OECD-2024-207",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-08-22",
        "countries": [
          "France",
          "Mexico"
        ],
        "ai_system": {
          "name": "Security/Surveillance Engine Pro",
          "version": "4.1.52"
        },
        "organizations": {
          "developer": "Uber",
          "deployer": "Apple"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "7671 customers",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Minority Groups",
          "Students",
          "Medical Residents"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2024-208",
        "title": "AI System Failure Related to Deepfake of Kamala Harris Saying Damaging Comments Circulates on X and Is Amplified by Elon Musk",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving X (Twitter). The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2024-09-03",
        "countries": [
          "Italy",
          "Brazil",
          "Sweden"
        ],
        "ai_system": {
          "name": "Smart Engine",
          "version": "2.2"
        },
        "organizations": {
          "developer": "X (Twitter)",
          "deployer": "X (Twitter)"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "47304 individuals",
          "economic_losses": "$6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Truth",
          "Kamala Harris",
          "Joe Biden",
          "General public",
          "American voters"
        ],
        "business_function": [
          "Compliance and justice",
          "Citizen/customer service"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/756"
      },
      {
        "incident_id": "OECD-2024-209",
        "title": "AI System Failure Related to AI-Generated Fake Audio of Verbal Abuse Incident Circulates of British Labour Leader Keir Starmer",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2024-09-03",
        "countries": [
          "United Kingdom",
          "Canada",
          "Australia"
        ],
        "ai_system": {
          "name": "AI Solution",
          "version": "1.2"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "17015 individuals",
          "economic_losses": "$4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "UK Labour Party",
          "Keir Starmer"
        ],
        "business_function": [
          "Logistics"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/601"
      },
      {
        "incident_id": "OECD-2024-210",
        "title": "Surgical Robot Malfunction During Minimally Invasive Procedure",
        "description": "An AI-assisted surgical robot experienced a control system failure during a procedure, requiring emergency conversion to traditional surgery. The AI's motion planning algorithms failed to account for unexpected tissue resistance.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Government Auditor",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-09-07",
        "countries": [
          "United Kingdom",
          "Netherlands"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "7.11.62"
        },
        "organizations": {
          "developer": "Northpointe",
          "deployer": "Starbucks"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical"
        ],
        "harm_quantification": {
          "affected_stakeholders": "50843 individuals",
          "economic_losses": "$7.0 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Patients",
          "Job applicants"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2024-211",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-09-11",
        "countries": [
          "Brazil"
        ],
        "ai_system": {
          "name": "Security/Surveillance Platform",
          "version": "6.7.37"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "New Zealand"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "39027 patients"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Job applicants",
          "Bus passengers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2024-212",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Whistleblower",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-09-12",
        "countries": [
          "Germany"
        ],
        "ai_system": {
          "name": "Security/Surveillance System Pro",
          "version": "5.15.73"
        },
        "organizations": {
          "developer": "Google",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "70577 patients",
          "economic_losses": "$40.4 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Bus passengers",
          "Workers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2024-213",
        "title": "AI System Failure Related to Starship’s Autonomous Food Delivery Robot Allegedly Stranded at Railroad Crossing in Oregon, Run over by Freight Train",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Oregon State University. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2024-09-16",
        "countries": [
          "Spain",
          "India"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "3.3"
        },
        "organizations": {
          "developer": "Oregon State University",
          "deployer": "Oregon State University"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31935 individuals",
          "economic_losses": "$7 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Oregon State University",
          "freight train crew"
        ],
        "business_function": [
          "Accounting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/176"
      },
      {
        "incident_id": "OECD-2024-214",
        "title": "AI System Failure Related to Glovo Driver in Italy Fired via Automated Email after Being Killed in Accident",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Glovo. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2024-10-07",
        "countries": [
          "Sweden",
          "India",
          "Japan"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "1.2"
        },
        "organizations": {
          "developer": "Glovo",
          "deployer": "Glovo"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "19496 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Sebastian Galassi's family",
          "Sebastian Galassi"
        ],
        "business_function": [
          "Marketing and advertisement",
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/384"
      },
      {
        "incident_id": "OECD-2024-215",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Contributing factor",
          "Human error"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-10-12",
        "countries": [
          "Brazil",
          "Japan"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "3.0.28"
        },
        "organizations": {
          "developer": "Equivant",
          "deployer": "Uber"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "18500 workers",
          "economic_losses": "$8.5 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Bus passengers",
          "Women",
          "Black people"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2024-216",
        "title": "AI System Failure Related to AI-Generated Fake Audio of Verbal Abuse Incident Circulates of British Labour Leader Keir Starmer",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-10-17",
        "countries": [
          "China",
          "Israel"
        ],
        "ai_system": {
          "name": "Intelligent Platform",
          "version": "3.4"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "11778 individuals",
          "economic_losses": "$8 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "UK Labour Party",
          "Keir Starmer"
        ],
        "business_function": [
          "Accounting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/601"
      },
      {
        "incident_id": "OECD-2024-217",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Safety Engineer",
          "email": "reporter@example.org",
          "affiliation": "Tech Accountability Project",
          "stakeholder_group": "I am an affected stakeholder",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-10-20",
        "countries": [
          "Israel",
          "Netherlands"
        ],
        "ai_system": {
          "name": "Medical AI Engine Pro",
          "version": "2.18.34"
        },
        "organizations": {
          "developer": "LinkedIn",
          "deployer": "New York city Dept. of Education"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "1622 patients",
          "economic_losses": "$26.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2024-218",
        "title": "AI System Failure Related to Bankrate's Resumption of AI-Generated Content Allegedly Continuing to Produce Inaccurate and Misleading Information",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Red Ventures. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-10-22",
        "countries": [
          "France",
          "China"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "2.9"
        },
        "organizations": {
          "developer": "Red Ventures",
          "deployer": "Red Ventures"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "39643 individuals",
          "economic_losses": "$5 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Journalistic integrity",
          "General public"
        ],
        "business_function": [
          "Research and development",
          "Maintenance"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/577"
      },
      {
        "incident_id": "OECD-2024-219",
        "title": "AI System Failure Related to British Female Politicians Victimized by Deepfake Pornography",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Unknown deepfake creators. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2024-11-07",
        "countries": [
          "India"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "1.7"
        },
        "organizations": {
          "developer": "Unknown deepfake creators",
          "deployer": "Unknown deepfake creators"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "4169 individuals",
          "economic_losses": "$6 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Stella Creasy",
          "Priti Patel",
          "Penny Mordaunt",
          "Gillian Keegan",
          "Dehenna Davison",
          "Angela Rayner"
        ],
        "business_function": [
          "Procurement",
          "Research and development"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/754"
      },
      {
        "incident_id": "OECD-2024-220",
        "title": "AI System Failure Related to Tesla Autopilot Misidentified On-Road Horse-Drawn Carriage",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Tesla. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-11-11",
        "countries": [
          "France"
        ],
        "ai_system": {
          "name": "Automated Engine",
          "version": "1.6"
        },
        "organizations": {
          "developer": "Tesla",
          "deployer": "Tesla"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "4910 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Tesla drivers",
          "horse-drawn carriages"
        ],
        "business_function": [
          "Marketing and advertisement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/398"
      },
      {
        "incident_id": "OECD-2024-221",
        "title": "AI System Failure Related to Bankrate's Resumption of AI-Generated Content Allegedly Continuing to Produce Inaccurate and Misleading Information",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Red Ventures. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-11-14",
        "countries": [
          "Singapore",
          "Sweden",
          "China"
        ],
        "ai_system": {
          "name": "Automated Platform",
          "version": "1.4"
        },
        "organizations": {
          "developer": "Red Ventures",
          "deployer": "Red Ventures"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "25267 individuals",
          "economic_losses": "$10 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Journalistic integrity",
          "General public"
        ],
        "business_function": [
          "Planning and budgeting",
          "Logistics"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/577"
      },
      {
        "incident_id": "OECD-2024-222",
        "title": "Predictive Policing Algorithm Creates Feedback Loops in Minority Neighborhoods",
        "description": "An AI system for predicting crime hotspots consistently directed more police presence to minority neighborhoods, creating a self-reinforcing cycle where increased police presence led to more reported incidents, further biasing the algorithm.",
        "ai_system_relation": [
          "Direct cause",
          "Failure to act"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "AI Safety Coalition",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-11-25",
        "countries": [
          "Netherlands",
          "South Korea"
        ],
        "ai_system": {
          "name": "Security/Surveillance Solution Pro",
          "version": "3.0.61"
        },
        "organizations": {
          "developer": "Youth Laboratories",
          "deployer": "Tesla"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Human or fundamental rights",
          "Psychological"
        ],
        "harm_quantification": {
          "affected_stakeholders": "63440 users",
          "psychological_impact": "Documented stress and anxiety in affected population"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Children"
        ],
        "business_function": [
          "Compliance and justice",
          "Planning and budgeting"
        ]
      },
      {
        "incident_id": "OECD-2024-223",
        "title": "Facial Recognition System Falsely Identifies Protesters as Criminals",
        "description": "A law enforcement facial recognition system incorrectly matched peaceful protesters with criminal database entries, leading to wrongful detentions. The AI system showed higher error rates for certain ethnic groups and in crowded scenarios.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Consumer Advocate",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-12-01",
        "countries": [
          "China",
          "France"
        ],
        "ai_system": {
          "name": "Security/Surveillance System Pro",
          "version": "10.9.5"
        },
        "organizations": {
          "developer": "Microsoft",
          "deployer": "Apple"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Physical",
          "Human or fundamental rights",
          "Reputational"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10108 patients",
          "economic_losses": "$13.3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Students",
          "Bus passengers",
          "Workers"
        ],
        "business_function": [
          "Compliance and justice"
        ]
      },
      {
        "incident_id": "OECD-2024-224",
        "title": "AI Diagnostic System Misses Critical Symptoms in Emergency Department",
        "description": "An AI triage system in emergency departments failed to identify critical symptoms in patients presenting with atypical manifestations of serious conditions. The system's training data lacked diversity in symptom presentations across different demographics.",
        "ai_system_relation": [
          "Direct cause",
          "Contributing factor"
        ],
        "submitter": {
          "role": "Data Scientist",
          "email": "reporter@example.org",
          "affiliation": "Consumer Protection Agency",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-12-14",
        "countries": [
          "United States"
        ],
        "ai_system": {
          "name": "Medical AI System",
          "version": "6.13.96"
        },
        "organizations": {
          "developer": "St George's Hospital Medical School",
          "deployer": "United States Government"
        },
        "severity": "Serious hazard",
        "harm_type": [
          "Physical",
          "Human or fundamental rights"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10058 users",
          "economic_losses": "$22.9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Medical Residents",
          "Women"
        ],
        "business_function": [
          "Other (healthcare services)"
        ]
      },
      {
        "incident_id": "OECD-2024-225",
        "title": "AI System Failure Related to Glovo Driver in Italy Fired via Automated Email after Being Killed in Accident",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Glovo. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2024-12-17",
        "countries": [
          "China",
          "United States"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "3.6"
        },
        "organizations": {
          "developer": "Glovo",
          "deployer": "Glovo"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "45220 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Sebastian Galassi's family",
          "Sebastian Galassi"
        ],
        "business_function": [
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/384"
      },
      {
        "incident_id": "OECD-2024-226",
        "title": "AI System Failure Related to Deepfake of Kamala Harris Saying Damaging Comments Circulates on X and Is Amplified by Elon Musk",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving X (Twitter). The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-12-23",
        "countries": [
          "Germany",
          "Japan"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "1.6"
        },
        "organizations": {
          "developer": "X (Twitter)",
          "deployer": "X (Twitter)"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "10636 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Truth",
          "Kamala Harris",
          "Joe Biden",
          "General public",
          "American voters"
        ],
        "business_function": [
          "Human resource management",
          "Sales"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/756"
      },
      {
        "incident_id": "OECD-2024-227",
        "title": "AI System Failure Related to Starship’s Autonomous Food Delivery Robot Allegedly Stranded at Railroad Crossing in Oregon, Run over by Freight Train",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Oregon State University. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2024-12-27",
        "countries": [
          "China"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "1.8"
        },
        "organizations": {
          "developer": "Oregon State University",
          "deployer": "Oregon State University"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "4668 individuals",
          "economic_losses": "$1 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Oregon State University",
          "freight train crew"
        ],
        "business_function": [
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/176"
      },
      {
        "incident_id": "OECD-2025-228",
        "title": "AI System Failure Related to British Female Politicians Victimized by Deepfake Pornography",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Unknown deepfake creators. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2025-01-23",
        "countries": [
          "Mexico"
        ],
        "ai_system": {
          "name": "Automated Platform",
          "version": "3.6"
        },
        "organizations": {
          "developer": "Unknown deepfake creators",
          "deployer": "Unknown deepfake creators"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "27655 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Stella Creasy",
          "Priti Patel",
          "Penny Mordaunt",
          "Gillian Keegan",
          "Dehenna Davison",
          "Angela Rayner"
        ],
        "business_function": [
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/754"
      },
      {
        "incident_id": "OECD-2025-229",
        "title": "AI System Failure Related to Class Action Lawsuit Over Alleged Defects in Volkswagen's AI-Driven AEB Systems",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Volkswagen Group of America. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-03-21",
        "countries": [
          "Netherlands",
          "Japan",
          "Brazil"
        ],
        "ai_system": {
          "name": "AI Engine",
          "version": "4.4"
        },
        "organizations": {
          "developer": "Volkswagen Group of America",
          "deployer": "Volkswagen Group of America"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "38477 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Volkswagen drivers",
          "Potential passengers and road users at risk due to malfunctioning AEB systems"
        ],
        "business_function": [
          "Marketing and advertisement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/746"
      },
      {
        "incident_id": "OECD-2025-230",
        "title": "AI System Failure Related to Bankrate's Resumption of AI-Generated Content Allegedly Continuing to Produce Inaccurate and Misleading Information",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Red Ventures. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-04-19",
        "countries": [
          "Singapore",
          "China"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "5.5"
        },
        "organizations": {
          "developer": "Red Ventures",
          "deployer": "Red Ventures"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "47290 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Journalistic integrity",
          "General public"
        ],
        "business_function": [
          "Research and development"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/577"
      },
      {
        "incident_id": "OECD-2025-231",
        "title": "AI System Failure Related to Female Celebrities' Faces Shown in Sexually Suggestive Ads for Deepfake App",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Facemega. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-04-23",
        "countries": [
          "Sweden",
          "Mexico",
          "Japan"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "1.5"
        },
        "organizations": {
          "developer": "Facemega",
          "deployer": "Facemega"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "45616 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Scarlett Johansson",
          "female celebrities",
          "Emma Watson"
        ],
        "business_function": [
          "Production",
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/494"
      },
      {
        "incident_id": "OECD-2025-232",
        "title": "AI System Failure Related to Female Celebrities' Faces Shown in Sexually Suggestive Ads for Deepfake App",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Facemega. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-05-26",
        "countries": [
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "2.7"
        },
        "organizations": {
          "developer": "Facemega",
          "deployer": "Facemega"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "6164 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Scarlett Johansson",
          "female celebrities",
          "Emma Watson"
        ],
        "business_function": [
          "Research and development"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/494"
      },
      {
        "incident_id": "OECD-2025-233",
        "title": "AI System Failure Related to Class Action Lawsuit Over Alleged Defects in Volkswagen's AI-Driven AEB Systems",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Volkswagen Group of America. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-05-29",
        "countries": [
          "Canada",
          "Japan"
        ],
        "ai_system": {
          "name": "Automated Solution",
          "version": "2.0"
        },
        "organizations": {
          "developer": "Volkswagen Group of America",
          "deployer": "Volkswagen Group of America"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "40484 individuals",
          "economic_losses": "$2 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Volkswagen drivers",
          "Potential passengers and road users at risk due to malfunctioning AEB systems"
        ],
        "business_function": [
          "ICT management and information security"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/746"
      },
      {
        "incident_id": "OECD-2025-234",
        "title": "AI System Failure Related to AI-Generated Fake Audio of Verbal Abuse Incident Circulates of British Labour Leader Keir Starmer",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Journalist",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-07-25",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "AI Solution",
          "version": "4.2"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "3172 individuals",
          "economic_losses": "$3 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "UK Labour Party",
          "Keir Starmer"
        ],
        "business_function": [
          "Monitoring and quality control",
          "Logistics"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/601"
      },
      {
        "incident_id": "OECD-2025-235",
        "title": "AI System Failure Related to AI-Generated Fake Audio of Verbal Abuse Incident Circulates of British Labour Leader Keir Starmer",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2025-08-22",
        "countries": [
          "United States",
          "United Kingdom",
          "India"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "3.9"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "13940 individuals",
          "economic_losses": "$4 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "UK Labour Party",
          "Keir Starmer"
        ],
        "business_function": [
          "Sales",
          "Maintenance"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/601"
      },
      {
        "incident_id": "OECD-2025-236",
        "title": "AI System Failure Related to Bankrate's Resumption of AI-Generated Content Allegedly Continuing to Produce Inaccurate and Misleading Information",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Red Ventures. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-09-27",
        "countries": [
          "Italy"
        ],
        "ai_system": {
          "name": "Intelligent Platform",
          "version": "3.4"
        },
        "organizations": {
          "developer": "Red Ventures",
          "deployer": "Red Ventures"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "45336 individuals",
          "economic_losses": "$7 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Journalistic integrity",
          "General public"
        ],
        "business_function": [
          "Logistics",
          "Citizen/customer service"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/577"
      },
      {
        "incident_id": "OECD-2025-237",
        "title": "AI System Failure Related to AI Chatbots Reportedly Inaccurately Conveyed Real-Time Political News",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Perplexity. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Government Inspector",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-10-10",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "Automated System",
          "version": "5.2"
        },
        "organizations": {
          "developer": "Perplexity",
          "deployer": "Perplexity"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "24992 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Journalism",
          "General public",
          "Chatbot users"
        ],
        "business_function": [
          "Accounting",
          "Sales"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/750"
      },
      {
        "incident_id": "OECD-2025-238",
        "title": "AI System Failure Related to AI-Generated Fake Audio of Verbal Abuse Incident Circulates of British Labour Leader Keir Starmer",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving unknown. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2025-11-28",
        "countries": [
          "Spain",
          "Japan"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "3.2"
        },
        "organizations": {
          "developer": "unknown",
          "deployer": "unknown"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "17797 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "UK Labour Party",
          "Keir Starmer"
        ],
        "business_function": [
          "Research and development",
          "Sales"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/601"
      },
      {
        "incident_id": "OECD-2025-239",
        "title": "AI System Failure Related to Bankrate's Resumption of AI-Generated Content Allegedly Continuing to Produce Inaccurate and Misleading Information",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Red Ventures. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "None of the above, but have partial or substantial knowledge of the incident (e.g. first-hand knowledge, research etc.)"
        },
        "date_of_occurrence": "2025-12-11",
        "countries": [
          "Germany",
          "Mexico"
        ],
        "ai_system": {
          "name": "AI System",
          "version": "5.4"
        },
        "organizations": {
          "developer": "Red Ventures",
          "deployer": "Red Ventures"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "16345 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Journalistic integrity",
          "General public"
        ],
        "business_function": [
          "Research and development",
          "Marketing and advertisement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/577"
      },
      {
        "incident_id": "OECD-2025-240",
        "title": "AI System Failure Related to Deepfake of Kamala Harris Saying Damaging Comments Circulates on X and Is Amplified by Elon Musk",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving X (Twitter). The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "University Research Lab",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2025-12-30",
        "countries": [
          "Spain"
        ],
        "ai_system": {
          "name": "AI Solution",
          "version": "1.3"
        },
        "organizations": {
          "developer": "X (Twitter)",
          "deployer": "X (Twitter)"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "25075 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Truth",
          "Kamala Harris",
          "Joe Biden",
          "General public",
          "American voters"
        ],
        "business_function": [
          "Planning and budgeting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/756"
      },
      {
        "incident_id": "OECD-2026-241",
        "title": "AI System Failure Related to AI Chatbots Reportedly Inaccurately Conveyed Real-Time Political News",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Perplexity. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2026-01-29",
        "countries": [
          "Netherlands"
        ],
        "ai_system": {
          "name": "AI System",
          "version": "4.9"
        },
        "organizations": {
          "developer": "Perplexity",
          "deployer": "Perplexity"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "37557 individuals",
          "economic_losses": "$9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Journalism",
          "General public",
          "Chatbot users"
        ],
        "business_function": [
          "Compliance and justice"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/750"
      },
      {
        "incident_id": "OECD-2026-242",
        "title": "AI System Failure Related to AI Chatbots Reportedly Inaccurately Conveyed Real-Time Political News",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Perplexity. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2026-02-21",
        "countries": [
          "South Korea"
        ],
        "ai_system": {
          "name": "AI System",
          "version": "2.2"
        },
        "organizations": {
          "developer": "Perplexity",
          "deployer": "Perplexity"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "47481 individuals",
          "economic_losses": "$1 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Journalism",
          "General public",
          "Chatbot users"
        ],
        "business_function": [
          "Production",
          "Procurement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/750"
      },
      {
        "incident_id": "OECD-2026-243",
        "title": "AI System Failure Related to British Female Politicians Victimized by Deepfake Pornography",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Unknown deepfake creators. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Academic Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2026-03-12",
        "countries": [
          "Australia",
          "France",
          "Canada"
        ],
        "ai_system": {
          "name": "Smart System",
          "version": "2.3"
        },
        "organizations": {
          "developer": "Unknown deepfake creators",
          "deployer": "Unknown deepfake creators"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31883 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Stella Creasy",
          "Priti Patel",
          "Penny Mordaunt",
          "Gillian Keegan",
          "Dehenna Davison",
          "Angela Rayner"
        ],
        "business_function": [
          "Human resource management",
          "Production"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/754"
      },
      {
        "incident_id": "OECD-2026-244",
        "title": "AI System Failure Related to Deepfake of Kamala Harris Saying Damaging Comments Circulates on X and Is Amplified by Elon Musk",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving X (Twitter). The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Journalist",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2026-04-19",
        "countries": [
          "Canada"
        ],
        "ai_system": {
          "name": "Smart Solution",
          "version": "4.4"
        },
        "organizations": {
          "developer": "X (Twitter)",
          "deployer": "X (Twitter)"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "46641 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Truth",
          "Kamala Harris",
          "Joe Biden",
          "General public",
          "American voters"
        ],
        "business_function": [
          "Procurement"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/756"
      },
      {
        "incident_id": "OECD-2026-245",
        "title": "AI System Failure Related to Deepfake of Kamala Harris Saying Damaging Comments Circulates on X and Is Amplified by Elon Musk",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving X (Twitter). The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "None of the above, but have partial or substantial knowledge of the incident",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2026-05-03",
        "countries": [
          "Canada"
        ],
        "ai_system": {
          "name": "Intelligent Solution",
          "version": "2.2"
        },
        "organizations": {
          "developer": "X (Twitter)",
          "deployer": "X (Twitter)"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "31658 individuals"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Truth",
          "Kamala Harris",
          "Joe Biden",
          "General public",
          "American voters"
        ],
        "business_function": [
          "Human resource management"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/756"
      },
      {
        "incident_id": "OECD-2026-246",
        "title": "AI System Failure Related to Class Action Lawsuit Over Alleged Defects in Volkswagen's AI-Driven AEB Systems",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Volkswagen Group of America. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2026-07-22",
        "countries": [
          "Mexico",
          "France",
          "Netherlands"
        ],
        "ai_system": {
          "name": "AI Solution",
          "version": "4.1"
        },
        "organizations": {
          "developer": "Volkswagen Group of America",
          "deployer": "Volkswagen Group of America"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "23934 individuals",
          "economic_losses": "$10 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Volkswagen drivers",
          "Potential passengers and road users at risk due to malfunctioning AEB systems"
        ],
        "business_function": [
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/746"
      },
      {
        "incident_id": "OECD-2026-247",
        "title": "AI System Failure Related to AI Chatbots Reportedly Inaccurately Conveyed Real-Time Political News",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Perplexity. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "Tech Policy Center",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2026-09-24",
        "countries": [
          "China",
          "Canada",
          "United States"
        ],
        "ai_system": {
          "name": "AI Solution",
          "version": "4.5"
        },
        "organizations": {
          "developer": "Perplexity",
          "deployer": "Perplexity"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "1659 individuals",
          "economic_losses": "$5 million"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Journalism",
          "General public",
          "Chatbot users"
        ],
        "business_function": [
          "Procurement",
          "Planning and budgeting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/750"
      },
      {
        "incident_id": "OECD-2026-248",
        "title": "AI System Failure Related to AI Chatbots Reportedly Inaccurately Conveyed Real-Time Political News",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Perplexity. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Industry Analyst",
          "email": "submitter@example.org",
          "affiliation": "Government Oversight Committee",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am an affected stakeholder"
        },
        "date_of_occurrence": "2026-10-04",
        "countries": [
          "United Kingdom"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "5.7"
        },
        "organizations": {
          "developer": "Perplexity",
          "deployer": "Perplexity"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "34695 individuals",
          "economic_losses": "$7 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Journalism",
          "General public",
          "Chatbot users"
        ],
        "business_function": [
          "Citizen/customer service",
          "Accounting"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/750"
      },
      {
        "incident_id": "OECD-2026-249",
        "title": "AI System Failure Related to Deepfake of Kamala Harris Saying Damaging Comments Circulates on X and Is Amplified by Elon Musk",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving X (Twitter). The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "Ethics Officer",
          "email": "submitter@example.org",
          "affiliation": "Digital Rights Foundation",
          "stakeholder_group": "I work or am affiliated to a public interest body or NGO",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2026-11-09",
        "countries": [
          "Mexico",
          "Canada"
        ],
        "ai_system": {
          "name": "Intelligent System",
          "version": "3.7"
        },
        "organizations": {
          "developer": "X (Twitter)",
          "deployer": "X (Twitter)"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "8267 individuals",
          "economic_losses": "$9 million"
        },
        "unintended_use": false,
        "affected_stakeholders": [
          "Truth",
          "Kamala Harris",
          "Joe Biden",
          "General public",
          "American voters"
        ],
        "business_function": [
          "ICT management and information security"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/756"
      },
      {
        "incident_id": "OECD-2026-250",
        "title": "AI System Failure Related to Class Action Lawsuit Over Alleged Defects in Volkswagen's AI-Driven AEB Systems",
        "description": "An AI system failure occurred with similar characteristics to the documented incident involving Volkswagen Group of America. The system exhibited comparable failure modes resulting in harm to affected stakeholders.",
        "ai_system_relation": [
          "Direct cause"
        ],
        "submitter": {
          "role": "AI Safety Researcher",
          "email": "submitter@example.org",
          "affiliation": "AI Safety Institute",
          "stakeholder_group": "I represent a government or regulatory body",
          "relation_to_incident": "I am a user of the related AI system"
        },
        "date_of_occurrence": "2026-11-25",
        "countries": [
          "South Korea",
          "Singapore",
          "Israel"
        ],
        "ai_system": {
          "name": "Intelligent Solution",
          "version": "5.0"
        },
        "organizations": {
          "developer": "Volkswagen Group of America",
          "deployer": "Volkswagen Group of America"
        },
        "severity": "Serious incident",
        "harm_type": [
          "Other (various)"
        ],
        "harm_quantification": {
          "affected_stakeholders": "28578 individuals"
        },
        "unintended_use": true,
        "affected_stakeholders": [
          "Volkswagen drivers",
          "Potential passengers and road users at risk due to malfunctioning AEB systems"
        ],
        "business_function": [
          "Monitoring and quality control"
        ],
        "sameAs": "https://incidentdatabase.ai/cite/746"
      }
    ],
    "metadata": {
      "total_incidents": 250,
      "aiid_linked": 81,
      "date_range": {
        "earliest": "2015-02-26",
        "latest": "2026-11-25"
      },
      "generation_date": "2025-07-31T23:43:21.027Z"
    }
  }
}